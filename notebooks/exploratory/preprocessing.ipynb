{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-25T09:51:12.724763Z",
     "start_time": "2025-07-25T09:51:12.720486Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "log_dir = Path('../../results/logs/preprocessing-logs')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-25T09:51:12.735107Z",
     "start_time": "2025-07-25T09:51:12.733277Z"
    }
   },
   "id": "2b72e576af88767",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-25T09:51:12.750977Z",
     "start_time": "2025-07-25T09:51:12.748837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_logs(log_dir, file_name='*.csv'):\n",
    "    log_dir = Path(log_dir)\n",
    "    csv_files = list(log_dir.rglob(file_name))\n",
    "    if not csv_files:\n",
    "        print('No CSV files found')\n",
    "        return None\n",
    "\n",
    "    all_dfs = list()\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['__source_file'] = csv_file.name\n",
    "        df['__log_type'] = (\n",
    "            'deduplication' if 'deduplication' in csv_file.name else\n",
    "            'near_identical' if 'near_identical' in csv_file.name else\n",
    "            'preprocessing'\n",
    "        )\n",
    "        all_dfs.append(df)\n",
    "    return pd.concat(all_dfs, ignore_index=True)"
   ],
   "id": "f86bae6d0dbc6cf8",
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "source": [
    "def analyze_preprocessing_logs(log_dir, category):\n",
    "    df = load_logs(log_dir, f\"./{category}/{category}_*_report.csv\")\n",
    "\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    df = df[df['__log_type'] == 'preprocessing']\n",
    "\n",
    "    # Convert numeric columns\n",
    "    df[\"GC Content (%)\"] = df[\"GC Content (%)\"].astype(float)\n",
    "    df[\"Avg Length\"] = df[\"Avg Length\"].astype(float)\n",
    "\n",
    "    # Summary counts\n",
    "    kept = df[df[\"Status\"] == \"KEPT\"]\n",
    "    deleted = df[df[\"Status\"] != \"KEPT\"]\n",
    "\n",
    "    print(f\"\\n===== Summary for {category.upper()} =====\")\n",
    "    print(f\"Total genomes processed: {len(df)}\")\n",
    "    print(f\" - KEPT: {len(kept)}\")\n",
    "    print(f\" - DELETED: {len(deleted)}\")\n",
    "\n",
    "    # Aggregate contig stats\n",
    "    total_contigs_before = df[\"Original Contigs\"].sum()\n",
    "    total_after_filter = df[\"After Filtering\"].sum()\n",
    "\n",
    "    print(f\"\\nContig Counts:\")\n",
    "    print(f\" - Original contigs: {total_contigs_before:,}\")\n",
    "    print(f\" - After filtering: {total_after_filter:,}\")\n",
    "\n",
    "    # GC and Length stats (for KEPT)\n",
    "    mean_gc = kept[\"GC Content (%)\"].mean() if not kept.empty else 0.0\n",
    "    mean_len = kept[\"Avg Length\"].mean() if not kept.empty else 0.0\n",
    "    total_bases = (kept[\"After Filtering\"] * kept[\"Avg Length\"]).sum()\n",
    "\n",
    "    print(f\"\\nStatistics for KEPT files:\")\n",
    "    print(f\" - Mean GC %: {mean_gc:.2f}\")\n",
    "    print(f\" - Mean Avg Length: {mean_len:.1f} bp\")\n",
    "    print(f\" - Total Bases Retained: {int(total_bases):,}\")\n",
    "\n",
    "    # Status breakdown\n",
    "    print(f\"\\nStatus breakdown:\")\n",
    "    print(df[\"Status\"].value_counts())\n",
    "\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-25T09:51:12.758654Z",
     "start_time": "2025-07-25T09:51:12.753161Z"
    }
   },
   "id": "61821a1f480fc1c1",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T09:51:12.766492Z",
     "start_time": "2025-07-25T09:51:12.763883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_deduplication_logs(log_dir, category):\n",
    "   df = load_logs(log_dir, f\"./{category}/{category}_*_report.csv\")\n",
    "   if df is None:\n",
    "       return None\n",
    "\n",
    "   df = df[df['__log_type'] == 'deduplication']\n",
    "   print(f\"\\n===== Deduplication Summary for {category.upper()} =====\")\n",
    "   if \"Split\" in df.columns:\n",
    "        print(\"\\nRemovals per split:\")\n",
    "        print(df[\"Split\"].value_counts())\n",
    "\n",
    "    # Count reasons for removal\n",
    "   if \"Reason\" in df.columns:\n",
    "        print(\"\\nReasons for removal:\")\n",
    "        print(df[\"Reason\"].value_counts())\n",
    "\n",
    "   return df\n"
   ],
   "id": "a18e2932376e892f",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T09:51:12.776708Z",
     "start_time": "2025-07-25T09:51:12.775136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_near_identical_logs(log_dir, category):\n",
    "    df = load_logs(log_dir, f\"{category}/{category}_near_identical_report.csv\")\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    df = df[df[\"__log_type\"] == \"near_identical\"]\n",
    "\n",
    "    print(f\"\\n===== Near-Identical Summary for {category.upper()} =====\")\n",
    "    if \"Split\" in df.columns:\n",
    "        print(\"\\nRemovals per split:\")\n",
    "        print(df[\"Split\"].value_counts())\n",
    "\n",
    "    return df\n"
   ],
   "id": "ec203bf8e21faeb8",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T09:51:12.788302Z",
     "start_time": "2025-07-25T09:51:12.785103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_log_per_category(log_dir, category):\n",
    "    preprocessing_df = analyze_preprocessing_logs(log_dir, category)\n",
    "    dedup_df = analyze_deduplication_logs(log_dir, category)\n",
    "    near_identical_df = analyze_near_identical_logs(log_dir, category)\n",
    "\n",
    "    return {\n",
    "        'preprocessing': preprocessing_df,\n",
    "        'deduplication': dedup_df,\n",
    "        'near_identical': near_identical_df,\n",
    "    }"
   ],
   "id": "677aff6bbbbe178",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T09:51:12.820646Z",
     "start_time": "2025-07-25T09:51:12.795543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "category = 'archaea'\n",
    "\n",
    "results = analyze_log_per_category(log_dir, category)"
   ],
   "id": "b52fba9efc863f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary for ARCHAEA =====\n",
      "Total genomes processed: 5885\n",
      " - KEPT: 5879\n",
      " - DELETED: 6\n",
      "\n",
      "Contig Counts:\n",
      " - Original contigs: 852,833.0\n",
      " - After filtering: 820,029.0\n",
      "\n",
      "Statistics for KEPT files:\n",
      " - Mean GC %: 46.39\n",
      " - Mean Avg Length: 257319.3 bp\n",
      " - Total Bases Retained: 9,951,876,385\n",
      "\n",
      "Status breakdown:\n",
      "Status\n",
      "KEPT                        5879\n",
      "DELETED (empty or error)       6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "===== Deduplication Summary for ARCHAEA =====\n",
      "\n",
      "Removals per split:\n",
      "Split\n",
      "test    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Reasons for removal:\n",
      "Reason\n",
      "Exact duplicate (MD5, Train vs Test)    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "===== Near-Identical Summary for ARCHAEA =====\n",
      "\n",
      "Removals per split:\n",
      "Split\n",
      "test    725\n",
      "val      58\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
